\documentclass{article}

\usepackage{listings}
\usepackage{color}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[nolist,nohyperlinks]{acronym}

\input{symbol.tex}
\input{acronym.tex}


\title{AIML-15: Machine Learning, Homework 2. Support Vector Machine and Model Selection}

\begin{document}



\maketitle

\paragraph{General information.} Problem solutions should be submitted in PDF format in report style (no source code listings required).
All reports must be submitted before December $20$ to the moodle (elearning) system.
It is advised to use Python as a programming language, but you can use any language of your choice (at your own risk).
In case you use Python, free \verb!Anaconda! distribution comes with all needed packages:
\begin{itemize}
\item[] \url{https://www.continuum.io/downloads}
\end{itemize}
In particular, you might find useful \verb!scikit-learn! general machine learning library and \verb!matplotlib! plotting facilities. When in doubt, read the manual and take a look at the large set of examples:
\begin{itemize}
\item[] \url{http://scikit-learn.org/stable/documentation.html}
\item[] \url{http://scikit-learn.org/stable/auto_examples/}
\item[] \url{http://matplotlib.org/examples/}
\end{itemize}

\paragraph{Data preparation.} In this homework you will work with the \verb!MNIST!~\cite{lecun1998mnist} dataset composed from $10$ classes of handwritten digits.
The dataset contains $\approx 70000$, $28\times 28$ images. Steps:
\begin{itemize}
\item If you are using Python and \verb!scikit-learn!, you can get training data by running:
\begin{lstlisting}[language=Python]
from sklearn.datasets import fetch_mldata
mnist = fetch_mldata('MNIST original')
X = mnist.data    # 70000 by 784 matrix of instances
y = mnist.target  # 70000 vector of labels
\end{lstlisting}
This might take some time  when you execute it for the first time, because this command will download the dataset.
\item If you are using Matlab, you can use~\url{http://www.cs.nyu.edu/~roweis/data/mnist_all.mat}.
\item Otherwise, in binary format from~\url{http://yann.lecun.com/exdb/mnist/}.
\end{itemize}

\paragraph{Training Linear \ac{SVM}.}
\begin{itemize}
\item Select the subset of $\bX$ and $\by$, which belongs only to classes $1$ and $7$.
\item Standardize, shuffle, and split selected data into the training, validation, and testing sets as $50\%, 20\%, 30\%$.
\item Train linear binary \ac{SVM}, varying parameter $C$ in the range of your choice, and plot it's accuracy on the \emph{validation} set against every choice of $C$.
In Python you can use \verb!scikit-learn!, e.g.,
\begin{lstlisting}
from sklearn.svm import LinearSVC
cls = LinearSVC(C=1)
cls.fit(X_train, y_train)    # Train on X_train, y_train
accuracy = cls.score(X_test, y_test)    # Test on X_test, y_test
\end{lstlisting}
and for plotting you can use \verb!pylab! library,
\begin{lstlisting}
import pylab; pylab.plot(C_range, accuracies)
\end{lstlisting}
\item Which $C$ will you use for the final classifier? Why?
\item Train linear binary \ac{SVM} once again, setting the best $C$. Test your classifier on the testing set and report obtained score.
\end{itemize}

\paragraph{Training Multiclass Non-Linear \ac{SVM}.}
\begin{itemize}
\item From $\bX$ and $\by$ select examples which belong only to classes $0,1,2,3,4$.
\item Standardize, shuffle, and split selected data into the training and testing sets as $50/50\%$ in a \emph{stratified} way.
Stratified means that all classes should be represented in both training and testing sets, according to the splitting ratio.
Consider label set $y = \{1,1,2,2,3,3,3,3\}$, where example of stratified splitting is , $y_{\text{train}} = \{1, 2, 3, 3\}$, and $y_{\text{test}} = \{1, 2, 3, 3\}$,
and a counter-example would be $y_{\text{train}} = \{1, 3, 3, 3\}$, and $y_{\text{test}} = \{2, 2, 3, 3\}$.
Why is it important to do stratified splitting?
\item Train a multiclass non-linear \ac{SVM} with Gaussian kernel function in \ac{OVA} way. You can train and get margin values of a binary non-linear \ac{SVM} like this:
\begin{lstlisting}
from sklearn.svm import SVC
cls = SVC(C=0.000001, kernel='rbf', gamma=10000)
cls.fit(X_train, y_train)    # Train on X_train, y_train
# Margin (decision) values of classifier
margins = cls.decision_function(X_test)    
\end{lstlisting}
\item How can you use it in for multiclass \ac{OVA} classification? Write down a short explanation.
\item Train multiclass non-linear \ac{SVM} and report the accuracy on the testing set.
\item Tune $C$ and $\gamma$ values of multiclass non-linear \ac{SVM} using the \emph{grid search}, to give the best performance. Remember, you cannot touch testing set during tuning!
Explain your steps for tuning these hyper-parameters.
Finally, when you have tuned everything, report the accuracy of the classifier on the testing set.
\end{itemize}

\bibliographystyle{plain}
\bibliography{common}

\end{document}